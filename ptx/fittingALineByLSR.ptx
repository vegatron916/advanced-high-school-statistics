<section xml:id="fittingALineByLSR">
  <title>Fitting a line by least squares regression</title>
  <introduction>
    <p>
          <idx><h>least squares regression</h></idx>
    </p>
    <p>
      In this section, we answer the following questions:
      <ul>
        <li>
          <p>
            How well can we predict financial aid based on family income for a particular college?
          </p>
        </li>
        <li>
          <p>
            How does one find, interpret,
            and apply the least squares regression line?
          </p>
        </li>
        <li>
          <p>
            How do we measure the fit of a model and compare different models to each other?
          </p>
        </li>
        <li>
          <p>
            Why do models sometimes make predictions that are ridiculous or impossible?
          </p>
        </li>
      </ul>
    </p>
  </introduction>
  <subsection>
    <title>Learning objectives</title>
    <ol>
      <li>
        <p>
          Calculate the slope and y-intercept of the least squares regression line using the relevant summary statistics.
          Interpret these quantities in context.
        </p>
      </li>
      <li>
        <p>
          Understand why the least squares regression line is called the least squares regression line.
        </p>
      </li>
      <li>
        <p>
          Interpret the explained variance <m>R^2</m>.
        </p>
      </li>
      <li>
        <p>
          Understand the concept of extrapolation and why it is dangerous.
        </p>
      </li>
      <li>
        <p>
          Identify outliers and influential points in a scatterplot.
        </p>
      </li>
    </ol>
  </subsection>
  <subsection>
    <title>An objective measure for finding the best line</title>
    <p>
      Fitting linear models by eye is open to criticism since it is based on an individual preference.
      In this section, we use <em>least squares regression</em>
      as a more rigorous approach.
    </p>
    <p>
      This section considers family income and gift aid data from a random sample of fifty students in the freshman class of Elmhurst College in Illinois.<fn>
      These data were sampled from a table of data for all freshmen from the 2011 class at Elmhurst College that accompanied an article titled
      <em>What Students Really Pay to Go to College</em>
      published online by <em>The<nbsp/>Chronicle of Higher Education</em>: \oiRedirect{textbook-chronicle_elmhurst_article}{chronicle.com/article/What-Students-Really-Pay-to-Go/131435}
      </fn> Gift aid is financial aid that does not need to be paid back,
      as opposed to a loan.
      A scatterplot of the data is shown in <xref ref="elmhurstScatterW2Lines">Figure</xref> along with two linear fits.
      The lines follow a negative trend in the data;
      students who have higher family incomes tended to have lower gift aid from the university.
    </p>
    <figure xml:id="elmhurstScatterW2Lines">
      <caption>Gift aid and family income for a random sample of 50 freshman students from Elmhurst College. Two lines are fit to the data, the solid line being the <em>least squares line</em>.</caption>
      <image width="70%" source="images/elmhurstScatterW2Lines.png" />
    </figure>
    <p>
      We begin by thinking about what we mean by
      <q>best</q>. Mathematically, we want a line that has small residuals.
      Perhaps our criterion could minimize the sum of the residual magnitudes:
      <md>
        <mrow>|y_1 - \hat{y}_1| + |y_2-\hat{y}_2| + \dots + |y_n-\hat{y}_n|</mrow>
      </md>
      which we could accomplish with a computer program.
      The resulting dashed line shown in <xref ref="elmhurstScatterW2Lines">Figure</xref>
      demonstrates this fit can be quite reasonable.
      However, a more common practice is to choose the line that minimizes the sum of the squared residuals:
      <md>
        <mrow>(y_1 - \hat{y}_1)^2 + (y_2-\hat{y}_2)^2+ \dots + (y_n-\hat{y}_n)^2</mrow>
      </md>
    </p>
    <p>
      The line that minimizes the sum of the squared residuals is represented as the solid line in <xref ref="elmhurstScatterW2Lines">Figure</xref>.
      This is commonly called the <term>least squares line</term>.
    </p>
    <p>
      Both lines seem reasonable,
      so why do data scientists prefer the least squares regression line?
      One reason is that it is easier to compute by hand and in most statistical software.
      Another, and more compelling,
      reason is that in many applications,
      a residual twice as large as another residual is more than twice as bad.
      For example,
      being off by 4 is usually more than twice as bad as being off by 2.
      Squaring the residuals accounts for this discrepancy.
    </p>
    <p>
      In <xref ref="leastSquares">Figure</xref>,
      we imagine the squared error about a line as actual squares.
      The least squares regression line minimizes the sum of the
      <em>areas</em> of these squared errors.
      In the figure, the sum of the squared error is <m>4+1+1=6</m>.
      There is no other line about which the sum of the squared error will be smaller.
    </p>
    <figure xml:id="leastSquares">
      <caption>A visualization of least squares regression using Desmos.  Try out this and other interactive Desmos activities at \oiRedirect{openintro-ahss-desmos}openintro.org/ahss/desmos.</caption>
      \oiRedirect{desmos-leastsquares}{
      <image width="75%" source="images/leastSquares.png" /> }
    </figure>
  </subsection>
  <subsection xml:id="findingTheLeastSquaresLineSection">
    <title>Finding the least squares line</title>
    <p>
      For the Elmhurst College data,
      we could fit a least squares regression line for predicting gift aid based on a student's family income and write the equation as:
      <md>
        <mrow>\widehat{\textit{aid}} = a  + b\times \textit{family_income}</mrow>
      </md>
    </p>
    <p>
      Here <m>a</m> is the <m>y</m>-intercept of the least squares regression line and <m>b</m> is the slope of the least squares regression line.
      <m>a</m> and <m>b</m> are both statistics that can be calculated from the data.
      In the next section we will consider the corresponding parameters that they statistics attempt to estimate.
    </p>
    <p>
      We can enter all of the data into a statistical software package and easily find the values of <m>a</m> and <m>b</m>.
      However, we can also calculate these values by hand,
      using only the summary statistics.
      <ul>
        <li>
          <p>
            The slope of the least squares line is given by
            <md>
              <mrow>b = r\frac{s_y}{s_x}</mrow>
            </md>
            where <m>r</m> is the correlation between the variables <m>x</m> and <m>y</m>,
            and <m>s_x</m> and <m>s_y</m> are the sample standard deviations of <m>x</m>,
            the explanatory variable, and <m>y</m>, the response variable.
          </p>
        </li>
        <li>
          <p>
            The point of averages <m>(\bar{x}, \bar{y})</m> is always on the least squares line.
            Plugging this point in for <m>x</m> and <m>y</m> in the least squares equation and solving for <m>a</m> gives
            <md>
              <mrow>\bar{y} \amp = a  + b\bar{x} \amp \amp a=\bar{y}-b\bar{x}</mrow>
            </md>
          </p>
        </li>
      </ul>
    </p>
    <assemblage>
      <title>Finding the slope and intercept of the least squares regression line</title>
      <p>
        The least squares regression line for predicting <m>y</m> based on <m>x</m> can be written as:
        <m>\hat{y}=a+bx</m>.
        <md>
          <mrow>b=r\frac{s_y}{s_x} \qquad \bar{y} = a + b\bar{x}</mrow>
        </md>
      </p>
      <p>
        We first find <m>b</m>, the slope,
        and then we solve for <m>a</m>, the <m>y</m>-intercept.
      </p>
    </assemblage>
    <exercise>
      <statement>
        <p>
          <xref ref="summaryStatsOfSATGPAData">Figure</xref>
          shows the sample means for the family income and gift aid as $101,800 and $19,940, respectively.
          Plot the point <m>(101.8, 19.94)</m> on <xref ref="elmhurstScatterW2Lines">Figure</xref>
          to verify it falls on the least squares line
          (the solid line).
        </p>
      </statement>
      <answer>
        <p>
          If you need help finding this location,
          draw a straight line up from the x-value of 100
          (or thereabout).
          Then draw a horizontal line at 20
          (or thereabout).
          These lines should intersect on the least squares line.
        </p>
      </answer>
    </exercise>
    <figure xml:id="summaryStatsOfSATGPAData">
      <caption>Summary statistics for family income and gift aid.</caption>
      <tabular>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{-4mm}</cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{0.4mm}</cell>
          <cell><nbsp/><nbsp/>family income, in $1000s (
            <q><m>x</m></q>
            )</cell>
          <cell><nbsp/><nbsp/>gift aid, in $1000s (
            <q><m>y</m></q>
            )</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{-3.9mm}</cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>mean</cell>
          <cell><m>\bar{x} = 101.8</m></cell>
          <cell><m>\bar{y} = 19.94</m></cell>
        </row>
        <row>
          <cell>sd</cell>
          <cell><m>s_x = 63.2</m></cell>
          <cell><m>s_y = 5.46</m>\vspace{0.4mm}</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{-4mm}<nbsp/></cell>
          <cell></cell>
        </row>
        <row>
          <cell></cell>
          <cell><m>r=-0.499</m></cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
      </tabular>
    </figure>
    <example>
      <statement>
        <p>
          Using the summary statistics in <xref ref="summaryStatsOfSATGPAData">Figure</xref>,
          find the equation of the least squares regression line for predicting gift aid based on family income.
        </p>
      </statement>
      <answer>
        <p>
          <md>
            <mrow>b \amp = r\frac{s_y}{s_x} = (-0.499)\frac{5.46}{63.2} = -0.0431</mrow>
            <mrow>a \amp  = \bar{y} - b\bar{x} = 19.94 - (-0.0431)(101.8) = 24.3</mrow>
            <mrow>\hat{y}\amp =24.3 - 0.0431x \qquad\text{ or } \qquad \widehat{\textit{aid}} = 24.3 - 0.0431\times \textit{family_income}</mrow>
          </md>
        </p>
      </answer>
    </example>
    <example>
      <statement>
        <p>
          Say we wanted to predict a student's family income based on the amount of gift aid that they received.
          Would this least squares regression line be the following?
          <md>
            <mrow>\textit{aid} = 24.3 - 0.0431\times \widehat{\textit{family_income}}</mrow>
          </md>
        </p>
      </statement>
      <answer>
        <p>
          No.
          The equation we found was for predicting aid,
          not for predicting family income.
          We would have to calculate a new regression line,
          letting <m>y</m> be <m>\textit{family_income}</m> and <m>x</m> be <m>\textit{aid}</m>.
          This would give us:
          <md>
            <mrow>b \amp = r\frac{s_y}{s_x} = (-0.499)\frac{63.2}{5.46} = -5.776</mrow>
            <mrow>a \amp  = \bar{y} - b\bar{x} = 19.94 - (-5.776)(101.8) = 607.9</mrow>
            <mrow>\hat{y}\amp =607.3 - 5.776x \qquad\text{ or } \qquad \widehat{\textit{family_income}} = 607.3 - 5.776\times \textit{aid}</mrow>
          </md>
        </p>
      </answer>
    </example>
    <p>
      We mentioned earlier that a computer is usually used to compute the least squares line.
      A summary table based on computer output is shown in <xref ref="rOutputForIncomeAidLSRLine">Figure</xref>
      for the Elmhurst College data.
      The first column of numbers provides estimates for <m>{b}_0</m> and <m>{b}_1</m>,
      respectively.
      Compare these to the result from <xref ref="findingTheSlopeOfTheLSRLineForIncomeAndAid">Example</xref>.
    </p>
    <figure xml:id="rOutputForIncomeAidLSRLine">
      <caption>Summary of least squares fit for the Elmhurst College data. Compare the parameter estimates in the first column to the results of Guided <xref ref="findingTheSlopeOfTheLSRLineForIncomeAndAid">Practice</xref>.</caption>
      <tabular>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{-3.7mm}</cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell></cell>
          <cell>Estimate</cell>
          <cell>Std. Error</cell>
          <cell>t value</cell>
          <cell>Pr(<m>></m><m>|</m>t<m>|</m>)</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{-3.6mm}</cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>(Intercept)</cell>
          <cell>24.3193</cell>
          <cell>1.2915</cell>
          <cell>18.83</cell>
          <cell>0.0000</cell>
        </row>
        <row>
          <cell>family_income</cell>
          <cell>-0.0431</cell>
          <cell>0.0108</cell>
          <cell>-3.98</cell>
          <cell>0.0002</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
      </tabular>
    </figure>
    <example>
      <statement>
        <p>
          Examine the second, third,
          and fourth columns in <xref ref="rOutputForIncomeAidLSRLine">Figure</xref>.
          Can you guess what they represent?
        </p>
      </statement>
      <answer>
        <p>
          We'll look at the second row,
          which corresponds to the slope.
          The first column, Estimate = -0.0431,
          tells us our best estimate for the slope of the population regression line.
          We call this point estimate <m>b</m>.
          The second column, Std.
          Error = 0.0108, is the standard error of this point estimate.
          The third column, t value = -3.98,
          is the <m>T</m> test statistic for the null hypothesis that the slope of the population regression line = 0.
          The last column, Pr(<m>></m><m>|</m>t<m>|</m>) = 0.0002,
          is the p-value for this two-sided <m>T</m>-test.
          We will get into more of these details in <xref ref="inferenceForLinearRegression">Section</xref>.
        </p>
      </answer>
    </example>
    <example>
      <statement>
        <p>
          Suppose a high school senior is considering Elmhurst College.
          Can she simply use the linear equation that we have found to calculate her financial aid from the university?
        </p>
      </statement>
      <solution>
        <p>
          No.
          Using the equation will provide a prediction or estimate.
          However, as we see in the scatterplot,
          there is a lot of variability around the line.
          While the linear equation is good at capturing the trend in the data,
          there will be significant error in predicting an individual student's aid.
          Additionally, the data all come from one freshman class,
          and the way aid is determined by the university may change from year to year.
        </p>
      </solution>
    </example>
  </subsection>
  <subsection>
    <title>Interpreting the coefficients of a regression line</title>
    <p>
          <idx><h>least squares regression</h><h>interpreting parameters</h></idx>
    </p>
    <p>
      Interpreting the coefficients in a regression model is often one of the most important steps in the analysis.
    </p>
    <example>
      <statement>
        <p>
          The slope for the Elmhurst College data for predicting gift aid based on family income was calculated as -0.0431.
          Intepret this quantity in the context of the problem.
        </p>
      </statement>
      <solution>
        <p>
          You might recall from an algebra course that slope is change in <m>y</m> over change in <m>x</m>.
          Here, both <m>x</m> and <m>y</m> are in thousands of dollars.
          So if <m>x</m> is one unit or one thousand dollars higher,
          the line will predict that <m>y</m> will change by 0.0431 thousand dollars.
          In other words,
          for each additional thousand dollars of family income,
          <em>on average</em>,
          students receive 0.0431 thousand,
          or $43.10 <em>less</em> in gift aid.
          Note that a higher family income corresponds to less aid because the slope is negative.
        </p>
      </solution>
    </example>
    <example>
      <statement>
        <p>
          The <m>y</m>-intercept for the Elmhurst College data for predicting gift aid based on family income was calculated as 24.3.
          Intepret this quantity in the context of the problem.
        </p>
      </statement>
      <solution>
        <p>
          The intercept <m>a</m> describes the predicted value of <m>y</m> when <m>x=0</m>.
          The <em>predicted</em> gift aid is 24.3 thousand dollars if a student's family has no income.
          The meaning of the intercept is relevant to this application since the family income for some students at Elmhurst is $0.
          In other applications,
          the intercept may have little or no practical value if there are no observations where <m>x</m> is near zero.
          Here, it would be acceptable to say that the <em>average</em>
          gift aid is 24.3 thousand dollars among students whose family have 0 dollars in income.
        </p>
      </solution>
    </example>
    <assemblage>
      <title>Interpreting coefficients in a linear model</title>
      <ul>
        <li>
          <p>
            The slope, <m>b</m>, describes the <em>average</em>
            increase or decrease in the <m>y</m> variable if the explanatory variable <m>x</m> is one unit larger.
          </p>
        </li>
        <li>
          <p>
            The y-intercept, <m>a</m>,
            describes the predicted outcome of <m>y</m> if <m>x=0</m>.
            The linear model must be valid all the way to <m>x=0</m> for this to make sense,
            which in many applications is not the case.
          </p>
        </li>
      </ul>
    </assemblage>
    <p>
          <idx><h>least squares regression</h><h>interpreting parameters</h></idx>
    </p>
    <exercise>
      <statement>
        <p>
          In the previous chapter,
          we encountered a data set that compared the price of new textbooks for UCLA courses at the UCLA Bookstore and on Amazon.
          We fit a linear model for predicting price at UCLA Bookstore from price on Amazon and we get:
          <md>
            <mrow>\hat{y} = 1.86 + 1.03x</mrow>
          </md>
          where <m>x</m> is the price on Amazon and <m>y</m> is the price at the UCLA bookstore.
          Interpret the coefficients in this model and discuss whether the interpretations make sense in this context.
        </p>
      </statement>
      <answer>
        <p>
          The <m>y</m>-intercept is 1.86 and the units of <m>y</m> are in dollars.
          This tells us that when a textbook costs 0 dollars on Amazon,
          the predicted price of the textbook at the UCLA Bookstore is 1.86 dollars.
          This does not make sense as Amazon does not sell any $0 textbooks.
          The slope is 1.03, with units (dollars)/(dollars).
          On average, for every extra dollar that a book costs on Amazon,
          it costs an extra 1.03 dollars at the UCLA Bookstore.
          This interpretation does make sense in this context.
        </p>
      </answer>
    </exercise>
    <exercise>
      <statement>
        <p>
          Can we conclude that if Amazon raises the price of a textbook by 1 dollar,
          the UCLA Bookstore will raise the price of the textbook by $1.03?
        </p>
      </statement>
      <answer>
        <p>
          No.
          The slope describes the overall trend.
          This is observational data; a causal conclusion cannot be drawn.
          Remember, a causal relationship can only be concluded by a well-designed randomized,
          controlled experiment.
          Additionally,
          there may be large variation in the points about the line.
          The slope does not tell us how much <m>y</m> might change based on a change in <m>x</m> for a particular textbook.
        </p>
      </answer>
    </exercise>
    <assemblage>
      <title>Exercise caution when interpreting coefficients of a linear model</title>
      <ul>
        <li>
          <p>
            The slope tells us only the <em>average</em>
            change in <m>y</m> for each unit change in <m>x</m>;
            it does not tell us how much <m>y</m> might change based on a change in <m>x</m> for any particular <em>individual</em>.
            Moreover, in most cases, the slope cannot be interpreted in a causal way.
          </p>
        </li>
        <li>
          <p>
            When a value of <m>x=0</m> doesn't make sense in an application,
            then the interpretation of the <m>y</m>-intercept won't have any practical meaning.
          </p>
        </li>
      </ul>
    </assemblage>
  </subsection>
  <subsection>
    <title>Extrapolation is treacherous</title>
    <p>
          <idx><h>least squares regression</h><h>extrapolation</h></idx>
    </p>
    <p>
      When those blizzards hit the East Coast this winter,
      it proved to my satisfaction that global warming was a fraud.
      That snow was freezing cold.
      But in an alarming trend, temperatures this spring have risen.
      Consider this: On February <m>6^{th}</m> it was 10 degrees.
      Today it hit almost 80.
      At this rate, by August it will be 220 degrees.
      So clearly folks the climate debate rages on.
    </p>
    <p>
      Stephen Colbert
    </p>
    <p>
      April 6th, 2010<fn>
      \oiRedirect{textbook-colbert_extrapolation}{www.cc.com/video-clips/l4nkoq/}
      </fn>
    </p>
    <p>
      Linear models can be used to approximate the relationship between two variables.
      However, these models have real limitations.
      Linear regression is simply a modeling framework.
      The truth is almost always much more complex than our simple line.
      For example,
      we do not know how the data outside of our limited window will behave.
    </p>
    <example>
      <statement>
        <p>
          Use the model <m>\widehat{\textit{aid}} = 24.3 - 0.0431\times \textit{family_income}</m> to estimate the aid of another freshman student whose family had income of $1 million.
        </p>
      </statement>
      <answer>
        <p>
          Recall that the units of family income are in $1000s,
          so we want to calculate the aid for <m>\textit{family_income}= 1000</m>:
          <md>
            <mrow>\widehat{\textit{aid}} \amp = 24.3 - 0.0431 \times \textit{family_income}</mrow>
            <mrow>\widehat{\textit{aid}}\amp =24.3 - 0.431(1000) = -18.8</mrow>
          </md>
        </p>
        <p>
          The model predicts this student will have -$18,800 in aid (!).
          Elmhurst College cannot
          (or at least does not)
          require any students to pay extra on top of tuition to attend.
        </p>
      </answer>
    </example>
    <p>
      Using a model to predict <m>y</m>-values for <m>x</m>-values outside the domain of the original data is called
      <term>extrapolation</term>.
      Generally, a linear model is only an approximation of the real relationship between two variables.
      If we extrapolate,
      we are making an unreliable bet that the approximate linear relationship will be valid in places where it has not been analyzed.
    </p>
    <p>
          <idx><h>least squares regression</h><h>extrapolation</h></idx>
    </p>
  </subsection>
  <subsection>
    <title>Using <m>R^2</m> to describe the strength of a fit</title>
    <p>
          <idx><h>least squares regression</h><h>R-squared (<m>R^2</m>)</h></idx>
    </p>
    <p>
      We evaluated the strength of the linear relationship between two variables earlier using the correlation,
      <m>r</m>.
      However, it is more common to explain the fit of a model using <m>R^2</m>,
      called <em>R-squared</em><idx><h>least squares regression</h><h>R-squared (<m>R^2</m>)|textbf</h></idx> or the
      <term>explained variance</term>.
      If provided with a linear model,
      we might like to describe how closely the data cluster around the linear fit.
    </p>
    <figure xml:id="elmhurstScatterWLSROnly">
      <caption>Gift aid and family income for a random sample of 50 freshman students from Elmhurst College, shown with the least squares regression line (<m>\hat{y}</m>) and the average line (<m>\bar{y}</m>).</caption>
      <image width="70%" source="images/elmhurstScatterWAveLine.png" />
    </figure>
    <p>
      We are interested in how well a model accounts for or explains the location of the <m>y</m> values.
      The <m>R^2</m> of a linear model describes how much smaller the variance
      (in the <m>y</m> direction)
      about the regression line is than the variance about the horizontal line <m>\bar{y}</m>.
      For example, consider the Elmhurst College data,
      shown in <xref ref="elmhurstScatterWLSROnly">Figure</xref>.
      The variance of the response variable,
      aid received, is <m>s_{aid}^2=29.8</m>.
      However, if we apply our least squares line,
      then this model reduces our uncertainty in predicting aid using a student's family income.
      The variability in the residuals describes how much variation remains after using the model:
      <m>s_{_{RES}}^2 = 22.4</m>.
      We could say that the reduction in the variance was:
      <me>
        \frac{s_{aid}^2 - s_{_{RES}}^2}{s_{aid}^2} = \frac{29.8 - 22.4}{29.8} = \frac{7.5}{29.8} = 0.25
      </me>
    </p>
    <p>
      If we used the simple standard deviation of the residuals,
      this would be exactly <m>R^2</m>.
      However, the standard way of computing the standard deviation of the residuals is slightly more sophisticated.<fn>
      In computing the standard deviation of the residuals,
      we divide by <m>n-2</m> rather than by <m>n-1</m> to account for the <m>n-2</m> degrees of freedom.
      </fn> To avoid any trouble,
      we can instead use a sum of squares method.
      If we call the sum of the squared errors about the regression line <m>SSRes</m> and the sum of the squared errors about the mean <m>SSM</m>,
      we can define <m>R^2</m> as follows:
      <md>
        <mrow>R^2=\frac{SSM - SSRes}{SSM} = 1-\frac{SSRes}{SSM}</mrow>
      </md>
    </p>
    <figure xml:id="rsq1">
      <caption>\subref{rsq1} The regression line is equivalent to <m>\bar{y}</m>; <m>R^2 = 0</m>.
      \subref{rsq2} The regression line passes through all of the points; <m>R^2=1</m>.  Try out this and other interactive Desmos activities at \oiRedirect{openintro-ahss-desmos}openintro.org/ahss/desmos.</caption>
      \oiRedirect{desmos-rsquared}{
      \Figures{0.4}
      {rSquared}
      {rsq1} }
      \Figures{0.4}
      {rSquared}
      {rsq2} }}
    </figure>
    <exercise>
      <statement>
        <p>
          Using the formula for <m>R^2</m>,
          confirm that in <xref ref="rSquared">Figure</xref> (a),
          <m>R^2 = 0</m> and that in <xref ref="rSquared">Figure</xref> (b),
          <m>R^2 = 1</m>.
        </p>
      </statement>
    </exercise>
    <assemblage>
      <title><m>R^2</m> is the explained variance</title>
      <p>
        <m>R^2</m> is always between 0 and 1, inclusive.
        It tells us the proportion of variation in the <m>y</m> values that is explained by a regression model.
        The higher the value of <m>R^2</m>, the better the model
        <q>explains</q>
        the response variable.
      </p>
    </assemblage>
    <p>
      The value of <m>R^2</m> is, in fact,
      equal to <m>r^2</m>, where <m>r</m> is the correlation.
      This means that <m>r = \pm \sqrt{R^2}</m>.
      Use this fact to answer the next two practice problems.
    </p>
    <exercise>
      <statement>
        <p>
          If a linear model has a very strong negative relationship with a correlation of -0.97,
          how much of the variation in the response variable is explained by the linear model?
        </p>
      </statement>
      <answer>
        <p>
          <m>R^2 = (-0.97)^2 = 0.94</m> or 94%. 94% of the variation in <m>y</m> is explained by the linear model.
        </p>
      </answer>
    </exercise>
    <p>
          <idx><h>least squares regression</h><h>R-squared (<m>R^2</m>)</h></idx>
    </p>
    <exercise>
      <statement>
        <p>
          If a linear model has an <m>R^2</m> or explained variance of 0.94, what is the correlation?
        </p>
      </statement>
    </exercise>
  </subsection>
  <subsection xml:id="calclinreg">
    <title>Calculator/Desmos: linear correlation and regression</title>
    <assemblage>
      <p>
        {MISSINGVIDEOLINK ti84_calculating_regression_summary_statistics TI-84:
        finding <m>{a}</m>, {<m>b</m>}, <m>R^2</m>,
        and {<m>r</m>} for a linear model} Use <c>STAT</c>, <c>CALC</c>, <c>LinReg(a + bx)</c>.
        <ol>
          <li>
            <p>
              Choose <c>STAT</c>.
            </p>
          </li>
          <li>
            <p>
              Right arrow to <c>CALC</c>.
            </p>
          </li>
          <li>
            <p>
              Down arrow and choose <c>8:LinReg(a+bx)</c>.
              <ul>
                <li>
                  <p>
                    Caution: choosing <c>4:LinReg(ax+b)</c> will reverse <m>a</m> and <m>b</m>.
                  </p>
                </li>
              </ul>
            </p>
          </li>
          <li>
            <p>
              Let <c>Xlist</c> be <c>L1</c> and <c>Ylist</c> be <c>L2</c> (don't forget to enter the <m>x</m> and <m>y</m> values in L1 and <c>L2</c> before doing this calculation).
            </p>
          </li>
          <li>
            <p>
              Leave <c>FreqList</c> blank.
            </p>
          </li>
          <li>
            <p>
              Leave <c>Store RegEQ</c> blank.
            </p>
          </li>
          <li>
            <p>
              Choose Calculate and hit <c>ENTER</c>,
              which returns:
              <tabular>
                <row>
                  <cell><c>a</c></cell>
                  <cell><m>a</m>, the y-intercept of the best fit line</cell>
                </row>
                <row>
                  <cell><c>b</c></cell>
                  <cell><m>b</m>, the slope of the best fit line</cell>
                </row>
                <row>
                  <cell><m>\textttmath{r^2}</m></cell>
                  <cell><m>R^2</m>, the explained variance</cell>
                </row>
                <row>
                  <cell><c>r</c></cell>
                  <cell><m>r</m>, the correlation coefficient</cell>
                </row>
              </tabular>
            </p>
          </li>
        </ol>
      </p>
      <p>
        TI-83: Do steps 1-3, then enter the <m>x</m> list and <m>y</m> list separated by a comma,
        e.g. <c>LinReg(a+bx) L1, L2</c>,
        then hit <c>ENTER</c>.
      </p>
    </assemblage>
    <assemblage>
      <p>
        {What to do if <m>r^2</m> and {<m>r</m>} do not show up on a TI-83/84} If <m>r^2</m> and <m>r</m> do now show up when doing <c>STAT</c>, <c>CALC</c>, <c>LinReg</c>, the
        <em>diagnostics</em> must be turned on.
        This only needs to be once and the diagnostics will remain on.
        <ol>
          <li>
            <p>
              Hit <c>2ND</c> <c>0</c> (i.e. <c>CATALOG</c>).
            </p>
          </li>
          <li>
            <p>
              Scroll down until the arrow points at <c>DiagnosticOn</c>.
            </p>
          </li>
          <li>
            <p>
              Hit <c>ENTER</c> and <c>ENTER</c> again.
              The screen should now say:
              <tabular>
                <row>
                  <cell><c>DiagnosticOn</c></cell>
                  <cell></cell>
                </row>
                <row>
                  <cell></cell>
                  <cell><c>Done</c></cell>
                </row>
              </tabular>
            </p>
          </li>
        </ol>
      </p>
    </assemblage>
    <assemblage>
      <p>
        {What to do if a TI-83/84 returns: {ERR:}<nbsp/>{DIM MISMATCH}} This error means that the lists,
        generally L1 and L2, do not have the same length.
        <ol>
          <li>
            <p>
              Choose <c>1:Quit</c>.
            </p>
          </li>
          <li>
            <p>
              Choose <c>STAT</c>,<nbsp/><c>Edit</c> and make sure that the lists have the same number of entries.
            </p>
          </li>
        </ol>
      </p>
    </assemblage>
    <assemblage>
      <p>
        {MISSINGVIDEOLINK casio_calculating_regression_summary_statistics Casio fx-9750GII: finding <m>{a}</m>, {<m>b</m>}, <m>R^2</m>,
        and {<m>r</m>} for a linear model}
        <ol>
          <li>
            <p>
              Navigate to <c>STAT</c> (<c>MENU</c> button,
              then hit the <c>2</c> button or select <c>STAT</c>).
            </p>
          </li>
          <li>
            <p>
              Enter the <m>x</m> and <m>y</m> data into 2 separate lists, e.g.
              <m>x</m> values in <c>List 1</c> and <m>y</m> values in <c>List 2</c>.
              Observation ordering should be the same in the two lists.
              For example, if <m>(5, 4)</m> is the second observation,
              then the second value in the <m>x</m> list should be 5 and the second value in the <m>y</m> list should be 4.
            </p>
          </li>
          <li>
            <p>
              Navigate to <c>CALC</c> (<c>F2</c>) and then <c>SET</c> (<c>F6</c>) to set the regression context.
              <ul>
                <li>
                  <p>
                    To change the <c>2Var XList</c>,
                    navigate to it,
                    select <c>List</c> (<c>F1</c>), and enter the proper list number.
                    Similarly, set <c>2Var YList</c> to the proper list.
                  </p>
                </li>
              </ul>
            </p>
          </li>
          <li>
            <p>
              Hit <c>EXIT</c>.
            </p>
          </li>
          <li>
            <p>
              Select <c>REG</c> (<c>F3</c>), <c>X</c> (<c>F1</c>), and <c>a+bx</c> (<c>F2</c>), which returns:
              <tabular>
                <row>
                  <cell><c>a</c></cell>
                  <cell><m>a</m>, the y-intercept of the best fit line</cell>
                </row>
                <row>
                  <cell><c>b</c></cell>
                  <cell><m>b</m>, the slope of the best fit line</cell>
                </row>
                <row>
                  <cell><c>r</c></cell>
                  <cell><m>r</m>, the correlation coefficient</cell>
                </row>
                <row>
                  <cell><m>\textttmath{r^2}</m></cell>
                  <cell><m>R^2</m>, the explained variance</cell>
                </row>
                <row>
                  <cell><c>MSe</c></cell>
                  <cell>Mean squared error, which you can ignore</cell>
                </row>
              </tabular>
              If you select <c>ax+b</c> (<c>F1</c>), the <c>a</c> and <c>b</c> meanings will be reversed.
            </p>
          </li>
        </ol>
      </p>
    </assemblage>
    <exercise xml:id="subsetOfLoan50">
      <statement>
        <p>
          The data set <c>loan50</c>,
          introduced in Chapter 1, contains information on randomly sampled loans offered through Lending Club.
          A subset of the data matrix is shown in <xref ref="data_for_regr_calc_exercise_loan50">Figure</xref>.
          Use a calculator to find the equation of the least squares regression line for predicting loan amount from total income.
        </p>
      </statement>
    </exercise>
    <figure xml:id="data_for_regr_calc_exercise_loan50">
      <caption>Sample of data from <c>loan50</c>.</caption>
      <tabular>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell></cell>
          <cell>total_income</cell>
          <cell>loan_amount</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>1</cell>
          <cell>59000</cell>
          <cell>22000</cell>
        </row>
        <row>
          <cell>2</cell>
          <cell>60000</cell>
          <cell>6000</cell>
        </row>
        <row>
          <cell>3</cell>
          <cell>75000</cell>
          <cell>25000</cell>
        </row>
        <row>
          <cell>4</cell>
          <cell>75000</cell>
          <cell>6000</cell>
        </row>
        <row>
          <cell>5</cell>
          <cell>254000</cell>
          <cell>25000</cell>
        </row>
        <row>
          <cell>6</cell>
          <cell>67000</cell>
          <cell>6400</cell>
        </row>
        <row>
          <cell>7</cell>
          <cell>28800</cell>
          <cell>3000</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
      </tabular>
    </figure>
    <example>
      <statement>
        <p>
          Use the full <c>loan50</c> data set (
          <url href="\oiRedirectUrl{data}">openintro.org/ahss/data) and this
          <url href="\oiRedirectUrl{desmos-linregcalculator}">Desmos Calculator</url>
          \mbox{({openintro.org/ahss/desmos})} to draw the scatterplot and find the equation of the least squares regression line for prediction loan amount (<m>y</m>) from total income (<m>x</m>).</url>
        </p>
      </statement>
      <answer>
        <p>
          <url href="\oiRedirectUrl{desmos-loan50scatter}"><image width="75%" source="images/loan50Desmos.png" /></url>
        </p>
      </answer>
    </example>
  </subsection>
  <subsection xml:id="typesOfOutliersInLinearRegression">
    <title>Types of outliers in linear regression</title>
    <p>
      Outliers in regression are observations that fall far from the
      <q>cloud</q>
      of points.
      These points are especially important because they can have a strong influence on the least squares line.
    </p>
    <example xml:id="outlierPlotsExample">
      <statement>
        <p>
          There are six plots shown in <xref ref="outlierPlots">Figure</xref>
          along with the least squares line and residual plots.
          For each scatterplot and residual plot pair,
          identify any obvious outliers and note how they influence the least squares line.
          Recall that an outlier is any point that doesn't appear to belong with the vast majority of the other points.
        </p>
      </statement>
      <answer>
        <ul>
          <li>
            <title>(1)</title>
            <p>
              There is one outlier far from the other points,
              though it only appears to slightly influence the line.
            </p>
          </li>
          <li>
            <title>(2)</title>
            <p>
              There is one outlier on the right,
              though it is quite close to the least squares line,
              which suggests it wasn't very influential.
            </p>
          </li>
          <li>
            <title>(3)</title>
            <p>
              There is one point far away from the cloud,
              and this outlier appears to pull the least squares line up on the right;
              examine how the line around the primary cloud doesn't appear to fit very well.
            </p>
          </li>
          <li>
            <title>(4)</title>
            <p>
              There is a primary cloud and then a small secondary cloud of four outliers.
              The secondary cloud appears to be influencing the line somewhat strongly,
              making the least squares line fit poorly almost everywhere.
              There might be an interesting explanation for the dual clouds,
              which is something that could be investigated.
            </p>
          </li>
          <li>
            <title>(5)</title>
            <p>
              There is no obvious trend in the main cloud of points and the outlier on the right appears to largely control the slope of the least squares line.
            </p>
          </li>
          <li>
            <title>(6)</title>
            <p>
              There is one outlier far from the cloud, however,
              it falls quite close to the least squares line and does not appear to be very influential.
            </p>
          </li>
        </ul>
      </answer>
    </example>
    <figure xml:id="outlierPlots">
      <caption>Six plots, each with a least squares line and residual plot. All data sets have at least one outlier.</caption>
      <image width="73%" source="images/outlierPlots.png" />
    </figure>
    <p>
      Examine the residual plots in <xref ref="outlierPlots">Figure</xref>.
      You will probably find that there is some trend in the main clouds of (3) and (4).
      In these cases,
      the outliers influenced the slope of the least squares lines.
      In (5), data with no clear trend were assigned a line with a large trend simply due to one outlier (!).
    </p>
    <assemblage>
      <title>Leverage</title>
      <p>
        Points that fall horizontally away from the center of the cloud tend to pull harder on the line,
        so we call them points with <term>high leverage</term>.
      </p>
    </assemblage>
    <p>
      Points that fall horizontally far from the line are points of high leverage;
      these points can strongly influence the slope of the least squares line.
      If one of these high leverage points does appear to actually invoke its influence on the slope of the line <mdash/> as in cases (3), (4),
      and (5) of <xref ref="outlierPlotsExample">Example</xref>
      <mdash/> then we call it an <term>influential point</term>.
      Usually we can say a point is influential if,
      had we fitted the line without it,
      the influential point would have been unusually far from the least squares line.
    </p>
    <p>
      It is tempting to remove outliers.
      Don't do this without a very good reason.
      Models that ignore exceptional
      (and interesting)
      cases often perform poorly.
      For instance,
      if a financial firm ignored the largest market swings <mdash/> the
      <q>outliers</q>
      <mdash/> they would soon go bankrupt by making poorly thought-out investments.
    </p>
    <assemblage>
      <title>Don't ignore outliers when fitting a final model</title>
      <p>
        {If there are outliers in the data,
        they should not be removed or ignored without a<nbsp/>good reason.
        Whatever final model is fit to the data would not be very helpful if it ignores the most exceptional cases.}
      </p>
    </assemblage>
  </subsection>
  <subsection xml:id="categoricalPredictorsWithTwoLevels">
    <title>Categorical predictors with two levels (special topic)</title>
    <p>
      Categorical variables are also useful in predicting outcomes.
      Here we consider a categorical predictor with two levels
      (recall that a <em>level</em> is the same as a <em>category</em>).
      We'll consider eBay auctions for a video game,
      <em>Mario Kart</em> for the Nintendo Wii,
      where both the total price of the auction and the condition of the game were recorded.<fn>
      These data were collected in Fall 2009 and may be found at \oiRedirect{textbook-openintro_org_stat}{openintro.org/stat}.
      </fn> Here we want to predict total price based on game condition,
      which takes values <c>used</c> and <c>new</c>.
      A plot of the auction data is shown in <xref ref="marioKartNewUsed">Figure</xref>.
    </p>
    <figure xml:id="marioKartNewUsed">
      <caption>Total auction prices for the game <em>Mario Kart</em>, divided into used (<m>x=0</m>) and new (<m>x=1</m>) condition games with the least squares regression line<nbsp/>shown.</caption>
      <image width="49%" source="images/marioKartNewUsed.png" />
    </figure>
    <p>
      To incorporate the game condition variable into a regression equation,
      we must convert the categories into a numerical form.
      We will do so using an <term>indicator variable</term>
      called <c>cond_new</c>,
      which takes value 1 when the game is new and 0 when the game is used.
      Using this indicator variable,
      the linear model may be written as
      <md>
        <mrow>\widehat{price} = \alpha + \beta \times \text{\texttt{cond_new}}</mrow>
      </md>
    </p>
    <p>
      The fitted model is summarized in <xref ref="marioKartNewUsedRegrSummary">Figure</xref>,
      and the model with its parameter estimates is given<nbsp/>as
      <md>
        <mrow>\widehat{price} = 42.87 + 10.90 \times \text{\texttt{cond_new}}</mrow>
      </md>
    </p>
    <p>
      For categorical predictors with two levels,
      the linearity assumption will always be satisfied.
      However, we must evaluate whether the residuals in each group are approximately normal with equal variance.
      Based on <xref ref="marioKartNewUsed">Figure</xref>,
      both of these conditions are reasonably satisfied.
    </p>
    <figure xml:id="marioKartNewUsedRegrSummary">
      <caption>Least squares regression summary for the Mario Kart data.</caption>
      <tabular>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{-3.7mm}</cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell></cell>
          <cell>Estimate</cell>
          <cell>Std. Error</cell>
          <cell>t value</cell>
          <cell>Pr(<m>></m><m>|</m>t<m>|</m>)</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>\vspace{-3.6mm}</cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>(Intercept)</cell>
          <cell>42.87</cell>
          <cell>0.81</cell>
          <cell>52.67</cell>
          <cell>0.0000</cell>
        </row>
        <row>
          <cell>cond_new</cell>
          <cell>10.90</cell>
          <cell>1.26</cell>
          <cell>8.66</cell>
          <cell>0.0000</cell>
        </row>
        <row bottom="minor">
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
      </tabular>
    </figure>
    <example>
      <statement>
        <p>
          Interpret the two parameters estimated in the model for the price of
          <em>Mario Kart</em> in eBay auctions.
        </p>
      </statement>
      <answer>
        <p>
          The intercept is the estimated price when <c>cond_new</c> takes value 0, i.e. when the game is in used condition.
          That<nbsp/>is,
          the average selling price of a used version of the game is $42.87.
        </p>
        <p>
          The slope indicates that, on average,
          new games sell for about $10.90 more than used games.
        </p>
      </answer>
    </example>
    <assemblage>
      <title>Interpreting model estimates for categorical predictors.</title>
      <p>
        The estimated intercept is the value of the response variable for the first category (i.e. the category corresponding to an indicator value of 0).
        The estimated slope is the average change in the response variable between the two categories.
      </p>
    </assemblage>
  </subsection>
  <subsection>
    <title>Section summary</title>
    <ul>
      <li>
        <p>
          We define the <em>best fit line</em>
          as the line that minimizes the sum of the squared residuals (errors) about the line.
          That<nbsp/>is,
          we find the line that minimizes <m>(y_1 - \hat{y}_1)^2 + (y_2-\hat{y}_2)^2+ \dots + (y_n-\hat{y}_n)^2=\sum{(y_i - \hat{y}_i)^2}</m>.
          We call this line the <term>least squares regression line</term>.
        </p>
      </li>
      <li>
        <p>
          We write the least squares regression line in the form:
          <m>\hat{y} = a  + bx</m>,
          and we can calculate <m>a</m> and <m>b</m> based on the summary statistics as follows:
          <md>
            <mrow>b=r\frac{s_y}{s_x} \qquad \text{ and }  \qquad a=\bar{y} - b\bar{x}</mrow>
          </md>.
        </p>
      </li>
      <li>
        <p>
          <em>Interpreting</em> the <term>slope</term>
          and <term>y-intercept</term> of a linear model
          <ul>
            <li>
              <p>
                The slope, <m>b</m>, describes the <em>average</em>
                increase or decrease in the <m>y</m> variable if the explanatory variable <m>x</m> is one unit larger.
              </p>
            </li>
            <li>
              <p>
                The y-intercept, <m>a</m>,
                describes the average or predicted outcome of <m>y</m> if <m>x=0</m>.
                The linear model must be valid all the way to <m>x=0</m> for this to make sense,
                which in many applications is not the case.
              </p>
            </li>
          </ul>
        </p>
      </li>
      <li>
        <p>
          Two important considerations about the regression line
          <ul>
            <li>
              <p>
                The regression line provides <em>estimates</em>
                or <em>predictions</em>, not actual values.
                It is important to know how large <m>s</m>,
                the standard deviation of the residuals,
                is in order to know about how much error to expect in these predictions.
              </p>
            </li>
            <li>
              <p>
                The regression line estimates are only reasonable within the domain of the data.
                Predicting <m>y</m> for <m>x</m> values that are outside the domain,
                known as <term>extrapolation</term>,
                is unreliable and may produce ridiculous results.
              </p>
            </li>
          </ul>
        </p>
      </li>
      <li>
        <p>
          Using <m>R^2</m> to assess the fit of the model
          <ul>
            <li>
              <p>
                <m>R^2</m>, called <term>R-squared</term>
                or the <term>explained variance</term>,
                is a measure of how well the model explains or fits the data.
                <m>R^2</m> is always between 0 and 1, inclusive,
                or between 0% and 100%, inclusive.
                The higher the value of <m>R^2</m>, the better the model
                <q>fits</q>
                the data.
              </p>
            </li>
            <li>
              <p>
                The <m>R^2</m> for a linear model describes the
                <em>proportion of variation</em>
                in the <m>y</m> variable that is
                <em>explained by</em> the regression line.
              </p>
            </li>
            <li>
              <p>
                <m>R^2</m> applies to any type of model, not just a linear model,
                and can be used to compare the fit among various models.
              </p>
            </li>
            <li>
              <p>
                The correlation <m>r = - \sqrt{R^2}</m> or <m>r = \sqrt{R^2}</m>.
                The value of <m>R^2</m> is always positive and cannot tell us the
                <em>direction</em> of the association.
                If finding <m>r</m> based on <m>R^2</m>,
                make sure to use either the scatterplot or the slope of the regression line to determine the
                <em>sign</em> of <m>r</m>.
              </p>
            </li>
          </ul>
        </p>
      </li>
      <li>
        <p>
          When a residual plot of the data appears as a random cloud of points,
          a linear model is generally appropriate.
          If a residual plot of the data has any type of pattern or curvature,
          such as a <m>\cup</m>-shape, a linear model is not appropriate.
        </p>
      </li>
      <li>
        <p>
          <em>Outliers</em>
            <idx><h>outlier|textbf</h></idx>
          in regression are observations that fall far from the
          <q>cloud</q>
          of points.
        </p>
      </li>
      <li>
        <p>
          An <term>influential point</term>
          is a point that has a big effect or pull on the slope of the regression line.
          Points that are outliers in the <m>x</m> direction will have more pull on the slope of the regression line and are more likely to be influential points.
        </p>
      </li>
    </ul>
  </subsection>
</section>